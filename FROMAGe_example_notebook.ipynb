{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bc991",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FROMAGe Visual Alignment Analysis\n",
    "## (https://arxiv.org/abs/2301.13823)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0652f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19b8018",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import logging, GPT2Tokenizer\n",
    "logging.set_verbosity_error()\n",
    "from fromage.models import FromageModel\n",
    "from fromage import utils\n",
    "from collections import namedtuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import distance\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4646a124",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = './fromage_model/'\n",
    "model_args_path = os.path.join(model_dir, 'model_args.json')\n",
    "model_ckpt_path = os.path.join(model_dir, 'pretrained_ckpt.pth.tar')\n",
    "with open(model_args_path, 'r') as f:\n",
    "  model_kwargs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3a2271",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_kwargs['opt_version'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Add special tokens to the model to enable [RET].\n",
    "tokenizer.add_special_tokens({\"cls_token\": \"<|image|>\"})\n",
    "tokenizer.add_tokens('[RET]')\n",
    "ret_token_idx = tokenizer('[RET]', add_special_tokens=False).input_ids\n",
    "assert len(ret_token_idx) == 1, ret_token_idx\n",
    "model_kwargs['retrieval_token_idx'] = ret_token_idx[0]\n",
    "args = namedtuple('args', model_kwargs)(**model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a3076",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547e6214",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuggingFace AutoFeatureExtractor for openai/clip-vit-large-patch14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\power\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using facebook/opt-6.7b for the language model.\n",
      "Using openai/clip-vit-large-patch14 for the visual model with 1 visual tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "772000b90ce34506a18cc9e29d3fb0f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing the LM.\n",
      "Initializing embedding for the retrieval token [RET] (id = 50266).\n",
      "Restoring pretrained weights for the visual model.\n",
      "Freezing the VM.\n"
     ]
    },
    {
     "data": {
      "text/plain": "FromageModel(\n  (lm): OPTForCausalLM(\n    (model): OPTModel(\n      (decoder): OPTDecoder(\n        (embed_tokens): Embedding(50267, 4096)\n        (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (12): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (13): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (14): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (15): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (16): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (17): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (18): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (19): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (20): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (21): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (22): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (23): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (24): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (25): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (26): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (27): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (28): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (29): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (30): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n          (31): OPTDecoderLayer(\n            (self_attn): OPTAttention(\n              (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n              (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n            (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (lm_head): Linear(in_features=4096, out_features=50267, bias=False)\n  )\n  (input_embeddings): Embedding(50267, 4096)\n  (visual_model): CLIPVisionModel(\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n        (position_embedding): Embedding(257, 1024)\n      )\n      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (12): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (13): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (14): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (15): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (16): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (17): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (18): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (19): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (20): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (21): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (22): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (23): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (text_hidden_fcs): ModuleList()\n  (visual_embeddings): Linear(in_features=1024, out_features=4096, bias=True)\n  (visual_fc): Linear(in_features=1024, out_features=256, bias=True)\n  (image_dropout): Dropout(p=0.0, inplace=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FromageModel(tokenizer=tokenizer, args=args)\n",
    "model.eval()\n",
    "model.half()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25e4c9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_model_from_ckpt(d):\n",
    "    new_dict = {}\n",
    "    for key, value in zip(d.keys(), d.values()):\n",
    "        if 'model' in key:\n",
    "            new_key = key.split('model.')[-1]\n",
    "        else:\n",
    "            new_key = key\n",
    "        new_dict[new_key] = d[key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e2c81a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_ckpt_path, map_location=torch.device('cpu'))\n",
    "checkpoint['state_dict'] = remove_model_from_ckpt(checkpoint['state_dict'])\n",
    "_ = model.load_state_dict(checkpoint['state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a6fb96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#no_projection\n",
    "def get_visual_embeddings(inp_image, mode='captioning', device='cpu', batch_size=-1):\n",
    "    if mode == 'captioning':\n",
    "        EMB_SIZE = 4096\n",
    "    else:\n",
    "        EMB_SIZE = 1024\n",
    "    model.visual_model.eval().to(device)\n",
    "    model.visual_embeddings.eval().to(device)\n",
    "    \n",
    "    if not isinstance(inp_image, list):\n",
    "        inp_image = [inp_image]\n",
    "        \n",
    "    if batch_size == -1:\n",
    "        batch_size = len(inp_image)\n",
    "    \n",
    "    if len(inp_image) / batch_size > len(inp_image) // batch_size:\n",
    "        one_more = True\n",
    "        remainder = len(inp_image) % batch_size\n",
    "    else:\n",
    "        one_more = False\n",
    "        remainder = 0\n",
    "    \n",
    "    iterations = len(inp_image) // batch_size\n",
    "    results = torch.empty(size=(len(inp_image), EMB_SIZE))\n",
    "    i = 0\n",
    "    for i in range(iterations):\n",
    "        storage = []\n",
    "        slice_ = inp_image[i*batch_size:(i+1)*batch_size]\n",
    "        for image in slice_:\n",
    "            storage.append(utils.get_pixel_values_for_model(model.feature_extractor, image)[None,...])\n",
    "\n",
    "        pixel_values = torch.concat(storage)\n",
    "        pixel_values = pixel_values.to(device=device, dtype=model.logit_scale.dtype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            visual_embs = model.get_visual_embs(pixel_values, mode=mode)\n",
    "        #print(f\"Side of left OP: {results[i*batch_size:(i+1)*batch_size, :].size()}\")\n",
    "        #print(f\"Side of right OP: {visual_embs.reshape(batch_size, 4096).cpu().size()}\")\n",
    "        results[i*batch_size:(i+1)*batch_size, :] = visual_embs.reshape(batch_size, EMB_SIZE).cpu()\n",
    "    if one_more:\n",
    "        storage = []\n",
    "        slice_ = inp_image[i*batch_size:i*batch_size + remainder]\n",
    "        for image in slice_:\n",
    "            storage.append(utils.get_pixel_values_for_model(model.feature_extractor, image)[None,...])\n",
    "\n",
    "        pixel_values = torch.concat(storage)\n",
    "        pixel_values = pixel_values.to(device=device, dtype=model.logit_scale.dtype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            visual_embs = model.get_visual_embs(pixel_values, mode=mode)\n",
    "        #print(f\"Side of left OP: {results[i*batch_size:(i+1)*batch_size, :].size()}\")\n",
    "        #print(f\"Side of right OP: {visual_embs.reshape(batch_size, 4096).cpu().size()}\")\n",
    "        results[i*batch_size:i*batch_size + remainder, :] = visual_embs.reshape(remainder, EMB_SIZE).cpu()\n",
    "        \n",
    "#     if device == 'cuda':\n",
    "#         torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e4704",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Embedding Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5e609",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LLM_embedding_matrix = model.lm.model.decoder.embed_tokens.weight # OPT 6.7B Embedding Matrix\n",
    "LLM_embedding_matrix = torch.Tensor(LLM_embedding_matrix.detach())\n",
    "LLM_embedding_matrix.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3705737",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lets produce position 1 embeddings for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b7e58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#LLM_embedding_matrix +=  model.lm.model.decoder.embed_positions(torch.ones(size=(1,2)))[0,1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9427d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Post Layer 1 Embedding Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb12873b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_similarity(a, top_k=20, verbose=True, low_rank=None, add_fuse_token=None, post_fuse_norm=False):\n",
    "    ### Returns the similarity between an embedding and the embedding matrix ###\n",
    "    ### Returns the top k (default k = 20) most similar tokens ###\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = torch.Tensor(a)\n",
    "    if low_rank is not None:\n",
    "        matrix = copy.deepcopy(low_rank)\n",
    "    else:\n",
    "        matrix = copy.deepcopy(LLM_embedding_matrix)\n",
    "        \n",
    "    if add_fuse_token is not None:\n",
    "        if '-' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: -x\n",
    "        elif '+' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: x\n",
    "        else:\n",
    "            fuse_index = int(add_fuse_token)\n",
    "            func = lambda x: x\n",
    "        \n",
    "        emb = func(matrix[fuse_index])\n",
    "        matrix = matrix + emb\n",
    "    \n",
    "    if post_fuse_norm:\n",
    "        matrix = torch.div(matrix, torch.norm(matrix, p=1, dim=1, keepdim=True))\n",
    "        \n",
    "    similarity = torch.nn.CosineSimilarity(dim=1)(a,matrix)\n",
    "    index_sorted = torch.argsort(similarity, descending=True)\n",
    "    top = index_sorted\n",
    "    if top_k != -1:\n",
    "        top = top[:top_k]\n",
    "    decoded_top = tokenizer.batch_decode(top)\n",
    "    if verbose:\n",
    "        print(decoded_top)\n",
    "    return top, decoded_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64b22b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_dp_similarity(a, top_k=20, verbose=True, low_rank=None, add_fuse_token=None, post_fuse_norm=False):\n",
    "    ### Returns the dot product between an embedding and the embedding matrix ###\n",
    "    ### Returns the top k (default k = 20) most similar tokens ###\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = torch.Tensor(a)\n",
    "    if low_rank is not None:\n",
    "        matrix = copy.deepcopy(low_rank)\n",
    "    else:\n",
    "        matrix = copy.deepcopy(LLM_embedding_matrix)\n",
    "        \n",
    "    if add_fuse_token is not None:\n",
    "        if '-' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: -x\n",
    "        elif '+' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: x\n",
    "        else:\n",
    "            fuse_index = int(add_fuse_token)\n",
    "            func = lambda x: x\n",
    "        \n",
    "        emb = func(matrix[fuse_index])\n",
    "        matrix = matrix + emb\n",
    "    \n",
    "    if post_fuse_norm:\n",
    "        matrix = torch.div(matrix, torch.norm(matrix, p=1, dim=1, keepdim=True))\n",
    "        \n",
    "    score = matrix @ a.T\n",
    "   \n",
    "    #score = score[:,0]\n",
    "    index_sorted = torch.argsort(score, descending=True)\n",
    "    top = index_sorted\n",
    "    if top_k != -1:\n",
    "        top = top[:top_k]\n",
    "    decoded_top = tokenizer.batch_decode(top)\n",
    "    if verbose:\n",
    "        print(decoded_top)\n",
    "    return top, decoded_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d88a6420",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_eucledian_similarity(a, norm=2, top_k=20, verbose=True, low_rank=None, add_fuse_token=None, post_fuse_norm=False):\n",
    "    ### Returns the reverse distance between an embedding and the embedding matrix ###\n",
    "    ### Returns the top k (default k = 20) most close tokens ###\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = torch.Tensor(a)\n",
    "    if low_rank is not None:\n",
    "        matrix = copy.deepcopy(low_rank)\n",
    "    else:\n",
    "        matrix = copy.deepcopy(LLM_embedding_matrix)\n",
    "        \n",
    "    if add_fuse_token is not None:\n",
    "        if '-' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: -x\n",
    "        elif '+' in add_fuse_token:\n",
    "            fuse_index = int(add_fuse_token[1:])\n",
    "            func = lambda x: x\n",
    "        else:\n",
    "            fuse_index = int(add_fuse_token)\n",
    "            func = lambda x: x\n",
    "        \n",
    "        emb = func(matrix[fuse_index])\n",
    "        matrix = matrix + emb\n",
    "    \n",
    "    if post_fuse_norm:\n",
    "        matrix = torch.div(matrix, torch.norm(matrix, p=1, dim=1, keepdim=True))\n",
    "        \n",
    "\n",
    "    distance = torch.cdist(matrix, a[None,...], p=norm)\n",
    "    distance = torch.flatten(distance)\n",
    "    index_sorted = torch.argsort(distance, descending=False)\n",
    "    top = index_sorted\n",
    "    if top_k != -1:\n",
    "        top = top[:top_k]\n",
    "    decoded_top = tokenizer.batch_decode(top)\n",
    "    if verbose:\n",
    "        print(decoded_top)\n",
    "    return top, decoded_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22753cd9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_rank(a, word, low_rank=None, add_fuse_token=None, post_fuse_norm=False):\n",
    "    _, decoded_tops_cos = get_embedding_similarity(a, -1, False, low_rank=low_rank, add_fuse_token=add_fuse_token, post_fuse_norm=post_fuse_norm)\n",
    "    _, decoded_tops_dp = get_dp_similarity(a, -1, False, low_rank=low_rank, add_fuse_token=add_fuse_token, post_fuse_norm=post_fuse_norm)\n",
    "    _, decoded_tops_l2 = get_eucledian_similarity(a, 2, -1, False, low_rank=low_rank, add_fuse_token=add_fuse_token, post_fuse_norm=post_fuse_norm)\n",
    "    (rank_cos, rank_dp, rank_euc) = (-1, -1, -1)\n",
    "    \n",
    "    for i, x in enumerate(decoded_tops_cos):\n",
    "        if word.lower() == x.strip().lower():\n",
    "            #print(f\"Cosine Similarity Rank: {i}\")\n",
    "            rank_cos = i\n",
    "            break\n",
    "            \n",
    "    for i, x in enumerate(decoded_tops_dp):\n",
    "        if word.lower() == x.strip().lower():\n",
    "            #print(f\"Cosine Similarity Rank: {i}\")\n",
    "            rank_dp = i\n",
    "            break\n",
    "            \n",
    "    for i, x in enumerate(decoded_tops_l2):\n",
    "        if word.lower() == x.strip().lower():\n",
    "            #print(f\"L2 Norm Rank: {i}\")\n",
    "            rank_euc = i\n",
    "            break\n",
    "    return rank_cos, rank_dp, rank_euc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee3d6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35bbcb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def low_rank_approx(a=None, r=1):\n",
    "    \"\"\"\n",
    "    Computes an r-rank approximation of a matrix\n",
    "    given the component u, s, and v of it's SVD\n",
    "    Requires: numpy\n",
    "    \"\"\"\n",
    "\n",
    "    u, s, v = np.linalg.svd(a, full_matrices=False)\n",
    "    Ar = np.zeros((len(u), len(v)))\n",
    "    for i in range(r):\n",
    "        Ar += s[i] * np.outer(u.T[i], v[i])\n",
    "    return Ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac4b91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Random Matrix\n",
    "# rm = LLM_embedding_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fd3d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7a04b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aprx = low_rank_approx(rm,r=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf41bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aprx = torch.Tensor(aprx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c01dfc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aprx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243a14a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aprx_l1 = torch.nn.functional.normalize(aprx, p=1, dim=1)\n",
    "# aprx_l2 = torch.nn.functional.normalize(aprx, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a04edb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiments with CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144c478",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cifar100\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399b294",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.sort(column='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841098e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_fish = dataset.filter(lambda example: example[\"coarse_label\"] == 1) #fish\n",
    "label_flower = dataset.filter(lambda example: example[\"coarse_label\"] == 2) #flowers\n",
    "label_people = dataset.filter(lambda example: example[\"coarse_label\"] == 14) # people\n",
    "label_fruit_and_veg = dataset.filter(lambda example: example[\"coarse_label\"] == 4) # fruit_and_vegetables\n",
    "label_vechicles = dataset.filter(lambda example: example[\"coarse_label\"] == 19) # vechicles\n",
    "label_small_animals = dataset.filter(lambda example: example[\"coarse_label\"] == 16) # small animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5b180",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_fish = dataset.filter(lambda example: example[\"fine_label\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f3214",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6af97",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    plt.imshow(label_fish[i]['img'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db8c6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set(label_fruit_and_veg['fine_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e88c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2: baby\n",
    "# 11: boy 500\n",
    "# 35: girl 500\n",
    "# 46: man 500\n",
    "# 98: woman 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c53bb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x: x==98, label_people['fine_label'])).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51151c8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Create Clusters from 200 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72172de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SIZE_OF_CLUSTERS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2f2d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8c7fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_single_color_images(n=1, color='black'):\n",
    "    assert color in ['blue','red','green','white','black','noise']\n",
    "    if color == 'blue':\n",
    "        RGB_vals = (0, 0, 255)\n",
    "    elif color == 'red':\n",
    "        RGB_vals = (255, 0, 0)\n",
    "    elif color == 'green':\n",
    "        RGB_vals = (0, 255, 0)\n",
    "    elif color == 'black':\n",
    "        RGB_vals = (0, 0, 0)\n",
    "    elif color == 'white':\n",
    "        RGB_vals = (255, 255, 255)\n",
    "    elif color == 'noise':\n",
    "        return [Image.fromarray(np.random.randint(0,255,(224,224,3),dtype=np.dtype('uint8'))) for _ in range(n)]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return Image.new('RGB', (224, 224), RGB_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92eb6d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "create_single_color_images(color='noise')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a8393",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#flower_centroid = get_visual_embeddings(label_flower['img'], batch_size=50, device='cuda').mean(dim=0)\n",
    "#people_centroid = get_visual_embeddings(label_people['img'], batch_size=50, device='cuda').mean(dim=0)\n",
    "#animal_centroid = get_visual_embeddings(label_small_animals['img'], batch_size=50, device='cuda').mean(dim=0)\n",
    "fruit_centroid = get_visual_embeddings(label_fruit_and_veg['img'], batch_size=50, device='cuda').mean(dim=0)\n",
    "fish_centroid = get_visual_embeddings(label_fish['img'], batch_size=50, device='cuda').mean(dim=0)\n",
    "# fnv_centroid = np.zeros(shape=(1,4096))\n",
    "# for i in range(SIZE_OF_CLUSTERS):\n",
    "#     ve_i = get_visual_embeddings(label_fruit_and_veg[i]['img'])\n",
    "#     fnv_centroid += ve_i.numpy()\n",
    "# fnv_centroid = fnv_centroid / SIZE_OF_CLUSTERS\n",
    "# vechicle_centroid = np.zeros(shape=(1,4096))\n",
    "# for i in range(SIZE_OF_CLUSTERS):\n",
    "#     ve_i = get_visual_embeddings(label_vechicles[i]['img'])\n",
    "#     vechicle_centroid += ve_i.numpy()\n",
    "# vechicle_centroid = vechicle_centroid / SIZE_OF_CLUSTERS\n",
    "# fish_centroid = np.zeros(shape=(1,4096))\n",
    "# for i in range(SIZE_OF_CLUSTERS):\n",
    "#     ve_i = get_visual_embeddings(label_fish[i]['img'])\n",
    "#     fish_centroid += ve_i.numpy()\n",
    "# fish_centroid = fish_centroid / SIZE_OF_CLUSTERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae500ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "white_centroid = get_visual_embeddings(create_single_color_images(color='white'), batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608599f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "black_centroid = get_visual_embeddings(create_single_color_images(color='black'), batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7cc8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_centroid = get_visual_embeddings(create_single_color_images(n=2, color='noise'), batch_size=2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e2716",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_centroid = random_centroid.mean(dim=0)[None,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0cd8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "black_centroid.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058641b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Global Text Centroid ###\n",
    "global_centroid = np.mean(LLM_embedding_matrix.numpy(), axis=0).reshape(1,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fa91a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3919bc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "people_centroid.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0416f18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ea0c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_bins = 100\n",
    "\n",
    "# Generate 3 \n",
    "dist1 = global_centroid[0]\n",
    "dist2 = random_centroid[0].numpy()\n",
    "dist3 = people_centroid.numpy()\n",
    "dist4 =animal_centroid.numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, sharey=True, tight_layout=True, figsize=(15, 15))\n",
    "\n",
    "axs[0].hist(dist1, bins=n_bins)\n",
    "axs[1].hist(dist2, bins=n_bins)\n",
    "axs[2].hist(dist3, bins=n_bins)\n",
    "axs[3].hist(dist4, bins=n_bins)\n",
    "axs[0].title.set_text('OPT 6.7 \\nembs')\n",
    "axs[1].title.set_text('Random Noise Image \\nembs')\n",
    "axs[2].title.set_text('People Image \\nembs')\n",
    "axs[3].title.set_text('Animal Image \\nembs')\n",
    "plt.savefig('difference.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f8aba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cluster Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1f299",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "WordChecklist = {\n",
    "#     'fish': {},\n",
    "     'animal': {},\n",
    "#     'water':{},\n",
    "#     'sea':{},\n",
    "#     'blue':{},\n",
    "     'flower':{},\n",
    "#     'plant':{},\n",
    "#     'pedal':{},\n",
    "#     'earth':{},\n",
    "#     'green':{},\n",
    "    '</s>':{},\n",
    "    'tree':{},\n",
    "     'man':{},\n",
    "#     'woman':{},\n",
    "#     'child':{},\n",
    "#     'people':{},\n",
    "#     'person':{},\n",
    "#     'fruit':{},\n",
    "#     'vegetable':{},\n",
    "#     'plant':{},\n",
    "#     'green':{},\n",
    "#     'car':{},\n",
    "#     'auto':{},\n",
    "#     'vehicle':{},\n",
    "#     'bike':{},\n",
    "#     'rat':{},\n",
    "#     'mouse':{},\n",
    "#     'hamster':{},\n",
    "#     'small':{},\n",
    "#     'squirrel':{}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddfb4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def experiment_of_similarities(wordchecklist=None,\n",
    "                               modulate_cluster_token=-1,\n",
    "                               add_fuse_token=-1,\n",
    "                               post_fuse_norm=False):\n",
    "    for cluster in [#('fish',fish_centroid),\n",
    "                #('flower',flower_centroid),\n",
    "                #('people', people_centroid),\n",
    "                #('fnv', fnv_centroid),\n",
    "                #('veh', vechicle_centroid),\n",
    "                #('sanim', animal_centroid),\n",
    "                ('black', black_centroid)\n",
    "               ]:\n",
    "        if modulate_cluster_token > -1:\n",
    "            modulator = copy.deepcopy(LLM_embedding_matrix[modulate_cluster_token])\n",
    "        for word in wordchecklist.keys():\n",
    "            if modulate_cluster_token > -1:\n",
    "                mod_a = torch.Tensor(cluster[1]) - modulator\n",
    "            else:\n",
    "                mod_a = torch.Tensor(cluster[1])\n",
    "                \n",
    "            \n",
    "            rank_cos, rank_dp, rank_euc = get_word_rank(a=mod_a, word=word, low_rank=None, add_fuse_token=add_fuse_token, post_fuse_norm=post_fuse_norm)\n",
    "            wordchecklist[word].update({f'cluster_{cluster[0]}': {'cos_rank': rank_cos, 'euc_rank': rank_euc, 'dp_rank':rank_dp}})\n",
    "    ## COS\n",
    "    data_pd = pd.DataFrame(wordchecklist)\n",
    "    data_pd_ = data_pd.applymap(lambda x: int(dict(x)['cos_rank']))\n",
    "    ordered_slice = pd.DataFrame(data=data_pd_.min().sort_values())\n",
    "    ordered_slice = ordered_slice.reset_index()\n",
    "    ordered_slice['tokenized'] = ordered_slice['index'].apply(lambda x: list(tokenizer.encode(x)))\n",
    "    ordered_slice['tokenized_len'] = ordered_slice['tokenized'].apply(lambda x: x.__len__())\n",
    "    ordered_slice = ordered_slice[ordered_slice['tokenized_len'] == 2]\n",
    "    ordered_slice_cos = ordered_slice.rename(columns={0:'cos_distance'})\n",
    "    \n",
    "    ## DP\n",
    "    data_pd = pd.DataFrame(wordchecklist)\n",
    "    data_pd_ = data_pd.applymap(lambda x: int(dict(x)['dp_rank']))\n",
    "    ordered_slice = pd.DataFrame(data=data_pd_.min().sort_values())\n",
    "    ordered_slice = ordered_slice.reset_index()\n",
    "    ordered_slice['tokenized'] = ordered_slice['index'].apply(lambda x: list(tokenizer.encode(x)))\n",
    "    ordered_slice['tokenized_len'] = ordered_slice['tokenized'].apply(lambda x: x.__len__())\n",
    "    ordered_slice = ordered_slice[ordered_slice['tokenized_len'] == 2]\n",
    "    ordered_slice_dp = ordered_slice.rename(columns={0:'dp_distance'})\n",
    "    \n",
    "    ## EUC\n",
    "    data_pd = pd.DataFrame(wordchecklist)\n",
    "    data_pd_ = data_pd.applymap(lambda x: int(dict(x)['euc_rank']))\n",
    "    ordered_slice = pd.DataFrame(data=data_pd_.min().sort_values())\n",
    "    ordered_slice = ordered_slice.reset_index()\n",
    "    ordered_slice['tokenized'] = ordered_slice['index'].apply(lambda x: list(tokenizer.encode(x)))\n",
    "    ordered_slice['tokenized_len'] = ordered_slice['tokenized'].apply(lambda x: x.__len__())\n",
    "    ordered_slice = ordered_slice[ordered_slice['tokenized_len'] == 2]\n",
    "    ordered_slice_euc = ordered_slice.rename(columns={0:'euc_distance'})\n",
    "    return ordered_slice_cos, ordered_slice_dp, ordered_slice_euc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef560f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e412b39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a862b0e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normal Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291e3a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_frame_cos, normal_frame_dp, normal_frame_euc = experiment_of_similarities(copy.deepcopy(WordChecklist), modulate_cluster_token=-1, add_fuse_token=None, post_fuse_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285f453",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_frame_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f4a84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_frame_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd1d9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_frame_euc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76753b8e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Additive FUSE \\</s> --> Idea add \\</s> (token nr.2) and recalculate embedding similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95707f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### V1: No Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ee8d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_cos, add_frame_dp, add_frame_euc = experiment_of_similarities(copy.deepcopy(WordChecklist),modulate_cluster_token=-1, add_fuse_token='2', post_fuse_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98192b25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2a8ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0f9c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_euc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd9ba7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### V2: Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c705c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_cos_n, add_frame_dp_n, add_frame_euc_n = experiment_of_similarities(copy.deepcopy(WordChecklist),modulate_cluster_token=-1, add_fuse_token='+2', post_fuse_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f0516",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_cos_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c0d6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_dp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e9a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_frame_euc_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0561b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Subtractive FUSE \\</s> --> Idea subtract \\</s> (token nr.2) from visual token and recalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb2ba2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_frame_cos, sub_frame_dp, sub_frame_euc = experiment_of_similarities(copy.deepcopy(WordChecklist),modulate_cluster_token=2, add_fuse_token='-1', post_fuse_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71745c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_frame_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916a586",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_frame_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040fc15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_frame_euc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c8993",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0d785",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#inp = tokenizer.encode('Repeat the word apple to me.', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae2924",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#oup = model.lm.generate(inp, max_new_tokens=2, return_dict_in_generate=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebb26b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#prompted_next_word = [f[0][0] for f in oup.hidden_states[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97d3e7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Can we invert language tokens to visual tokens? Not really..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f87b1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# W = model.visual_embeddings.weight\n",
    "# W.requires_grad = False\n",
    "# b = model.visual_embeddings.bias\n",
    "# b.requires_grad = False\n",
    "# import numpy\n",
    "# W_inv = numpy.linalg.pinv(W.numpy())\n",
    "# (W @ W_inv).mean()\n",
    "# LLM_embedding_matrix.shape\n",
    "# W_inv.shape\n",
    "# LLM_embedding_matrix_inv = (LLM_embedding_matrix - b) @ W_inv.T\n",
    "# LLM_embedding_matrix_inv.shape\n",
    "# LLM_embedding_matrix_inv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200651a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Layer FUSE --> Idea add \\</s> (token nr.2) and pass token through 1 layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a80547",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_llm_embeds(words=None, add_s=True):\n",
    "    save_dict = {k:[] for k in words}\n",
    "    with torch.no_grad():\n",
    "        for word in words:\n",
    "            inps = tokenizer.encode(word, return_tensors='pt')\n",
    "            start_idx = 1\n",
    "            if add_s == False:\n",
    "                start_idx = 0\n",
    "                inps = inps[:,1:]\n",
    "            output = model.lm(inps, output_hidden_states=True)\n",
    "            save_dict[word].append([output.hidden_states[i][0,start_idx,:] for i in range(len(output.hidden_states))])\n",
    "    save_dict = {k:v[0] for k,v in save_dict.items()}\n",
    "    return save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4106c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visual_emb = black_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f1192",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mixed_llm_embeds(visual_emb=None, text_emb=None, text=None):\n",
    "    with torch.no_grad():\n",
    "        if text is not None:\n",
    "            text_id = tokenizer.encode(text, return_tensors='pt')\n",
    "            text_emb = model.lm.model.decoder.embed_tokens(text_id) #+ model.lm.model.decoder.embed_positions(torch.ones_like(text_id))\n",
    "        else:\n",
    "            pass\n",
    "        if visual_emb is not None:\n",
    "            if not isinstance(visual_emb, list):\n",
    "                embs = torch.concat([visual_emb[None,...], text_emb], dim=1)\n",
    "            else:\n",
    "                emb = [ve[None,...] for ve in visual_emb]\n",
    "                emb.append(text_emb)\n",
    "                embs = torch.concat(emb, dim=1)\n",
    "        else:\n",
    "            embs =  text_emb\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a74d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_embedding_change(og_embed, emb_mat_list):\n",
    "    \"\"\"\n",
    "        Gets as input a list of tensors and need to calculate the inbetween l2_norm differences\n",
    "    \"\"\"\n",
    "    total_number = len(emb_mat_list)\n",
    "    similarity_change = []\n",
    "    norm_change = []\n",
    "    for i in range(total_number):\n",
    "        sim = torch.nn.CosineSimilarity(dim=0)(og_embed, emb_mat_list[i])\n",
    "        #sim = og_embed @ emb_mat_list[i].T\n",
    "        diff = torch.norm(\n",
    "            (emb_mat_list[i] - og_embed),\n",
    "            p=2)\n",
    "        similarity_change.append(sim)\n",
    "        norm_change.append(diff)\n",
    "    return similarity_change, norm_change    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df26063",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lets try the extraction experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584e313",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_apple = utils.get_image_from_url(\n",
    "    'https://media.istockphoto.com/id/184276818/photo/red-apple.jpg?s=612x612&w=0&k=20&c=NvO-bLsG0DJ_7Ii8SSVoKLurzjmV0Qi4eGfn6nW3l5w='\n",
    ")\n",
    "plt.imshow(inp_image_apple)\n",
    "apple_vector = get_visual_embeddings(inp_image_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536c248",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 1: Get an centroid e.g humans\n",
    "visual_embedding = apple_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff97c0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 2: Join in with a potential caption or another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fff8c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mixed_embs = get_mixed_llm_embeds(visual_emb = visual_embedding, text='Picture of an')\n",
    "print(f\"Total seq length: {mixed_embs.size()[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1362ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Step 3: Pass it through all layers and get the hidden states\n",
    "with torch.no_grad():\n",
    "    output = model.lm(inputs_embeds=mixed_embs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ca63d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_org = [output.hidden_states[i][0,:,:] for i in range(len(output.hidden_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42394e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img_hs = [f[0,:] for f in hidden_states_org]\n",
    "img_plus_start_hs = [f[1,:] for f in hidden_states_org]\n",
    "man_hs =  [f[2,:] for f in hidden_states_org]\n",
    "tree_hs =  [f[3,:] for f in hidden_states_org]\n",
    "dog_hs =  [f[4,:] for f in hidden_states_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cf4d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### IS ANY HS in the normal range of an embedding? -> Not really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2d795",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fast_cossim(x):\n",
    "    cos = lambda m: F.normalize(m, p=2, dim=-1) @ F.normalize(m, p=2, dim=-1).t()\n",
    "    return torch.stack([cos(m) for m in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a5c9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fast_ecd(x):\n",
    "    return torch.cdist(x,x,p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6026736",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b108d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(33):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(fast_ecd(hidden_states_org[i][None,...]))\n",
    "    print(fast_cossim(hidden_states_org[i][None,...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecad797",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Removing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0365d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mixed_embs = get_mixed_llm_embeds(visual_emb = black_centroid, text='Picture of an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7e9fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total seq length: {mixed_embs.size()[1]}\")\n",
    "### Step 3: Pass it through all layers and get the hidden states\n",
    "with torch.no_grad():\n",
    "    output = model.lm(inputs_embeds=mixed_embs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c298f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_blk = [output.hidden_states[i][0,:,:] for i in range(len(output.hidden_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223727f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img_hs_blk = [f[0,:] for f in hidden_states_blk]\n",
    "img_plus_start_hs_blk = [f[1,:] for f in hidden_states_blk]\n",
    "man_hs_blk =  [f[2,:] for f in hidden_states_blk]\n",
    "tree_hs_blk =  [f[3,:] for f in hidden_states_blk]\n",
    "dog_hs_blk =  [f[4,:] for f in hidden_states_blk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dc7be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(33):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(fast_ecd(hidden_states_blk[i][None,...]) - fast_ecd(hidden_states_org[i][None,...]))\n",
    "    print(fast_cossim(hidden_states_blk[i][None,...]) - fast_cossim(hidden_states_org[i][None,...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502c28b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### What about Noise as Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439a7ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mixed_embs = get_mixed_llm_embeds(visual_emb = random_centroid, text='Picture of an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e410",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total seq length: {mixed_embs.size()[1]}\")\n",
    "### Step 3: Pass it through all layers and get the hidden states\n",
    "with torch.no_grad():\n",
    "    output = model.lm(inputs_embeds=mixed_embs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb31dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_noise = [output.hidden_states[i][0,:,:] for i in range(len(output.hidden_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513c6be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img_hs_ns = [f[0,:] for f in hidden_states_noise]\n",
    "img_plus_start_hs_ns = [f[1,:] for f in hidden_states_noise]\n",
    "man_hs_ns =  [f[2,:] for f in hidden_states_noise]\n",
    "tree_hs_ns =  [f[3,:] for f in hidden_states_noise]\n",
    "dog_hs_ns =  [f[4,:] for f in hidden_states_noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d9817",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(33):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(fast_ecd(hidden_states_noise[i][None,...]) - fast_ecd(hidden_states_blk[i][None,...]))\n",
    "    print(fast_cossim(hidden_states_noise[i][None,...]) - fast_cossim(hidden_states_blk[i][None,...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03513782",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Is maybe the alignment happening in a further inside layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e251208",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mixed_embs = get_mixed_llm_embeds(visual_emb = apple_vector, text='Picture of a red apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02d3be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total seq length: {mixed_embs.size()[1]}\")\n",
    "### Step 3: Pass it through all layers and get the hidden states\n",
    "with torch.no_grad():\n",
    "    output = model.lm(inputs_embeds=mixed_embs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfc8f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_apple = [output.hidden_states[i][0,:,:] for i in range(len(output.hidden_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9515e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "apple_vector.min(), apple_vector.max(), apple_vector.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f189f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Closest are HL1 and HL32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9ec9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Combine apple with hd\n",
    "visual_apple_hs1 = torch.concat([apple_vector, hidden_states_apple[1]], dim=0)\n",
    "visual_apple_hs2 = torch.concat([apple_vector, hidden_states_apple[2]], dim=0)\n",
    "visual_apple_hs3 = torch.concat([apple_vector, hidden_states_apple[3]], dim=0)\n",
    "visual_apple_hs31 = torch.concat([apple_vector, hidden_states_apple[31]], dim=0)\n",
    "visual_apple_hs32 = torch.concat([apple_vector, hidden_states_apple[32]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488ec48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#fast_cossim(visual_apple_hs1[None,...])\n",
    "fast_ecd(visual_apple_hs1[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840e3c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#fast_cossim(visual_apple_hs31[None,...])\n",
    "fast_ecd(visual_apple_hs32[None,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6f3ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Are images of the same class converging or diverging through layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecc542",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Get 2 random noise images\n",
    "random_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f5e9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "person_1 = utils.get_image_from_url('https://www.athensvoice.gr/images/1074x600/jpg/files/2023-02-01/rotting-christ-sakis-tolis-interview__5_.jpg')\n",
    "person_2 = utils.get_image_from_url('https://i.wpimg.pl/1200x/d.wpimg.pl/1819922225--1211360234/till-lindemann-rammstein.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2d7ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Get 2 human images\n",
    "two_people = get_visual_embeddings([person_1,person_2], batch_size=2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d4794",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "animal_1 = utils.get_image_from_url('https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/golden-retriever-royalty-free-image-506756303-1560962726.jpg?crop=0.672xw:1.00xh;0.166xw,0&resize=640:*')\n",
    "animal_2 = utils.get_image_from_url('https://i.insider.com/5abd339d3216741b008b459e?width=1000&format=jpeg&auto=webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b2e21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Get 2 animal images\n",
    "two_animals = get_visual_embeddings([animal_1,animal_2], batch_size=2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5992441",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mixed_embs = get_mixed_llm_embeds(visual_emb=[random_centroid,two_people,two_animals], text='man human animal dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090a33b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total seq length: {mixed_embs.size()[1]}\")\n",
    "### Step 3: Pass it through all layers and get the hidden states\n",
    "with torch.no_grad():\n",
    "    output = model.lm(inputs_embeds=mixed_embs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0194cd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_diverge = [output.hidden_states[i][0,:,:] for i in range(len(output.hidden_states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7a1a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_states_diverge[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30ab93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a952e",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(33):\n",
    "    #print(f\"Layer {i}:\")\n",
    "    #print(fast_ecd(hidden_states_diverge[i][None,...])[0])\n",
    "    #print(fast_cossim(hidden_states_diverge[i][None,...])[0])\n",
    "    value = fast_cossim(hidden_states_diverge[i][None,...])[0].numpy()\n",
    "    annots = ['Noise_1', 'Noise_2','Human_1','Human_2','Dog_1','Dog_2','BOS_token','man<text>', 'human<text>', 'animal<text>','dog<text>']\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.set_title(f\"Layer {i}:\")\n",
    "    im, cbar = heatmap(value, annots , annots, ax=ax,\n",
    "                       cmap=\"YlGn\", cbarlabel=\"Cosine Similarity\")\n",
    "    texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a522e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How separable are concepts before and after the Projection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fd3f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Case 1: Cifar-100\n",
    "### Step-1 :Sample N images from Cifar-100\n",
    "### Step-2 :Perform Feature extraction from CLIP-VIT-base\n",
    "### Step-3 :Perform only Feature*W+B with w,b from FROMAGE\n",
    "### Step-4 :Perform T-SNE/Umap on Step2 and on Step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c4c66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d031caf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.sort(column='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30280f88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_map = np.zeros(\n",
    "    (\n",
    "        N*len(set(dataset['coarse_label'])),\n",
    "        1024\n",
    "    )\n",
    ")\n",
    "\n",
    "fromage_feature_map = np.zeros(\n",
    "    (\n",
    "        N*len(set(dataset['coarse_label'])),\n",
    "        4096\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2109e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_map = np.load('cifar100_500_coarse_examples_no_proj.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cc60e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fromage_feature_map = np.load('cifar100_500_coarse_examples_fromage_proj.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ccbcf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dba01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for coarse_idx, coarse_label in enumerate(set(dataset['coarse_label'])):\n",
    "#     temp_data = dataset.filter(lambda example: example[\"coarse_label\"] == coarse_label)['img'][0:N]\n",
    "#     embs = get_visual_embeddings(temp_data, mode='no_projection', device='cuda', batch_size=50).cpu()\n",
    "#     feature_map[N*coarse_idx:N*(coarse_idx+1),:] = embs\n",
    "# W = model.visual_embeddings.weight.detach().cpu().numpy()\n",
    "# B = model.visual_embeddings.bias.detach().cpu().numpy()\n",
    "# fromage_feature_map = feature_map @ W.T + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9dfb09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e979cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac10d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efb5da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Perform T-SNE\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6a904",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#np.save('cifar100_500_coarse_examples_no_proj.npy', feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d117c56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#np.save('cifar100_500_coarse_examples_fromage_proj.npy', fromage_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72534674",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = scale(feature_map,axis=0)\n",
    "pca = PCA(n_components=300, random_state=1995)\n",
    "pca.fit(features)\n",
    "feature_map_pca = pca.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2766c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### To explain the same amount of variance in 2 representations the same amount of eigenvectors is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c325ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca6ee0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3,\n",
    "            learning_rate='auto', \n",
    "            perplexity=30,\n",
    "            angle=0.2, \n",
    "            verbose=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=1995).fit_transform(feature_map_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e1dfe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tx, ty, tz = tsne[:,0], tsne[:,1], tsne[:,2]\n",
    "tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx))\n",
    "ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))\n",
    "tz = (tz-np.min(tz)) / (np.max(tz) - np.min(tz))\n",
    "projections = np.stack([tx,ty,tz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe724bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "projections = np.stack([tx,ty,tz], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045e8e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cifar_100_coarse_classes = {0: 'aquatic_mammals',\n",
    "                            1: 'fish',\n",
    "                            2: 'flowers',\n",
    "                            3: 'food_containers',\n",
    "                            4: 'fruit_and_vegetables',\n",
    "                            5: 'household_electrical_devices',\n",
    "                            6: 'household_furniture',\n",
    "                            7: 'insects',\n",
    "                            8: 'large_carnivores',\n",
    "                            9: 'large_man-made_outdoor_things',\n",
    "                            10: 'large_natural_outdoor_scenes',\n",
    "                            11: 'large_omnivores_and_herbivores',\n",
    "                            12: 'medium_mammals',\n",
    "                            13: 'non-insect_invertebrates',\n",
    "                            14: 'people',\n",
    "                            15: 'reptiles',\n",
    "                            16: 'small_mammals',\n",
    "                            17: 'trees',\n",
    "                            18: 'vehicles_1',\n",
    "                            19: 'vehicles_2'\n",
    "                           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02944dac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "color_array = []\n",
    "for i in range(20):\n",
    "    for _ in range(N):\n",
    "        color_array.append(i)\n",
    "        \n",
    "color_array = [f if f!=19 else 18 for f in color_array]\n",
    "label_array = [cifar_100_coarse_classes[f] for f in color_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99886c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    projections, x=0, y=1, z=2,\n",
    "    color=label_array,\n",
    ")\n",
    "fig.update_traces(marker_size=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b5b37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    projections, x=0, y=1, z=2,\n",
    "    color=label_array,\n",
    ")\n",
    "fig.update_traces(marker_size=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a4aae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d52b0594",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lets try the blockage experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cca090",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Image of the 224x224 animal\n",
    "animal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461ca74",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ed65f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def patch_blackout(img=None, ten=None, kernel_size=32, overlap_size=16, show=False, specific_ids=None):\n",
    "    \"\"\"\n",
    "        Blacksout part of the image or tensor\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    if img is not None:\n",
    "        ten = transforms.ToTensor()(img)\n",
    "    \n",
    "    ss = []\n",
    "    k = 0\n",
    "    if specific_ids is not None:\n",
    "        for s in specific_ids:\n",
    "            s_x = (s // ((ten.size()[1] // overlap_size) - 3)) + 1\n",
    "            s_y = (s %  ((ten.size()[1] // overlap_size) - 3)) + 1\n",
    "            print(s_x,s_y)\n",
    "            ss.append((s_x, s_y))\n",
    "        for i,j in ss:\n",
    "            temp = copy.deepcopy(ten)\n",
    "            x_idx = i * overlap_size\n",
    "            y_idx = j * overlap_size\n",
    "            temp[:,\n",
    "              x_idx:x_idx + kernel_size,\n",
    "              y_idx:y_idx + kernel_size\n",
    "             ] = 0\n",
    "            temp_pil = transforms.functional.to_pil_image(temp)\n",
    "            ret.append(temp_pil)\n",
    "            if show:\n",
    "                plt.title(f'Patch ID: {specific_ids[k]}, i:{i}, j:{j}')\n",
    "                k += 1\n",
    "                plt.imshow(temp_pil)\n",
    "                plt.show()\n",
    "        embs = get_visual_embeddings(ret, mode='captioning', device='cuda', batch_size=min(50, len(ret)))\n",
    "        return embs\n",
    "    else:\n",
    "        for i in range(1,(ten.size()[1] // overlap_size) - 2):\n",
    "            for j in range (1,(ten.size()[2] // overlap_size) - 2):\n",
    "                temp = copy.deepcopy(ten)\n",
    "                x_idx = i * overlap_size\n",
    "                y_idx = j * overlap_size\n",
    "                temp[:,\n",
    "                  x_idx:x_idx + kernel_size,\n",
    "                  y_idx:y_idx + kernel_size\n",
    "                 ] = 0\n",
    "                temp_pil = transforms.functional.to_pil_image(temp)\n",
    "                ret.append(temp_pil)\n",
    "                if show:\n",
    "                    plt.title(f'Patch ID: {len(ret)-1}, i:{i}, j:{j}')\n",
    "                    plt.imshow(temp_pil)\n",
    "                    plt.show()\n",
    "\n",
    "        #Pack to tensor\n",
    "        embs = get_visual_embeddings(ret, mode='captioning', device='cuda', batch_size=50)\n",
    "        return embs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90daee",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs = patch_blackout(animal_2, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432415a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_faulty_patch(embs, look_only_at=None, prompt='Picture of a', target_class='dog', device='cpu'):\n",
    "    for emb_idx in range(embs.size()[0]):\n",
    "        if look_only_at is not None:\n",
    "            if emb_idx in look_only_at:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        prompts = [embs[emb_idx,:],prompt]\n",
    "        input_embs = []\n",
    "        add_bos = False\n",
    "        for i, p in enumerate(prompts):\n",
    "            if type(p) == str:\n",
    "                text_ids = model.tokenizer(p,\n",
    "                                           add_special_tokens=True,\n",
    "                                           return_tensors=\"pt\").input_ids.to(device)\n",
    "                text_embs = model.input_embeddings(text_ids)\n",
    "                input_embs.append(text_embs)\n",
    "            elif type(p) == torch.Tensor:\n",
    "                input_embs.append(p[None, None, ...])\n",
    "            else:\n",
    "                raise ValueError(f'Input prompts should be either PIL.Image.Image or str types, got {type(p)} instead.')\n",
    "        \n",
    "        input_embs = torch.cat(input_embs, dim=1)\n",
    "        input_embs.to(device)\n",
    "        generated_ids, generated_embeddings, _ = model.generate(input_embs,\n",
    "                                                                20,\n",
    "                                                                temperature=0.0,\n",
    "                                                                top_p=1,\n",
    "                                                                ret_scale_factor=1)\n",
    "        embeddings = generated_embeddings[-1][:, input_embs.shape[1]:]\n",
    "        newline_token_id = model.tokenizer('\\n', add_special_tokens=False).input_ids[0]\n",
    "        trunc_idx = 0\n",
    "        for j in range(generated_ids.shape[1]):\n",
    "            if generated_ids[0, j] == newline_token_id:\n",
    "                trunc_idx = j\n",
    "                break\n",
    "        if trunc_idx > 0:\n",
    "            generated_ids = generated_ids[:, :trunc_idx]\n",
    "            embeddings = embeddings[:, :trunc_idx]\n",
    "        return_outputs = []\n",
    "        caption = model.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return_outputs.append(utils.truncate_caption(caption))\n",
    "        print(f\"Patch: {emb_idx}:\\n\")\n",
    "        print(return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e9f59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "find_faulty_patch(embs, look_only_at=[0,1,2,28,29,30,31,40,41,42,100,110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b3aed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs = patch_blackout(animal_2, show=False, specific_ids=[0, 28, 30, 31, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df8f4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232e7d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Locate Dog Face ###\n",
    "    #print(fast_ecd(hidden_states_diverge[i][None,...])[0])\n",
    "    #print(fast_cossim(hidden_states_diverge[i][None,...])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0fa96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value = fast_ecd(embs[None,...])[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aeccba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe1fc8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value = fast_cossim(embs[None,...])[0].numpy()\n",
    "annots = ['Dog No Mask 1', 'Mask Near Dog','Mask on Dog 1','Mask on Dog 2','Dog No Mask 2']\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "im, cbar = heatmap(value, annots , annots, ax=ax,\n",
    "                   cmap=\"YlGn\", cbarlabel=\"Cosine Similarity\")\n",
    "texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a76ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Which dims are responsible for \"Dog Face?\" ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18fd5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "og_dog = get_visual_embeddings(animal_2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a99a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs_mask = patch_blackout(animal_2, show=False, specific_ids=[29,30,31,40]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046fe48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936157",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs_no_mask_1 = patch_blackout(animal_2, show=False, specific_ids=[0,1,2,3,4,5,6,7,100,101,102,103,104,105,106,107]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaacebc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#20,21,22,23,24,25,26,27,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769e16b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embs_no_mask_2 = patch_blackout(animal_2, show=False, specific_ids=[50,51,52,53,54,55,56,57]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541f535",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_dimensions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6b079",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f0513",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_dimensions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd148f9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff_1 = (embs_no_mask_1 - embs_mask)**2\n",
    "a = diff_1.argsort(descending=True)[0:n_dimensions]\n",
    "diff_2 = (embs_no_mask_2 - embs_mask)**2\n",
    "b = diff_2.argsort(descending=True)[0:n_dimensions]\n",
    "diff_3 = (og_dog - embs_mask)**2\n",
    "c = diff_3.argsort(descending=True)[0:n_dimensions]\n",
    "d = set(a.numpy()).intersection(set(b.numpy())).intersection(set(c.numpy()))\n",
    "print(len(d), len(d) / n_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2c981",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_dimensions = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75979a59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff_1 = (embs_no_mask_1 - embs_mask)**2\n",
    "a = diff_1.argsort(descending=False)[0:n_dimensions]\n",
    "diff_2 = (embs_no_mask_2 - embs_mask)**2\n",
    "b = diff_2.argsort(descending=False)[0:n_dimensions]\n",
    "c = set(a.numpy()).intersection(set(b.numpy()))\n",
    "print(len(c), len(c) / n_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79d5ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Gather Top-5 most influencial dimensions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e31fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b67b4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4815ddf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc00965",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd953d15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534c0d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "646a80ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lets try KNN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b03be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# og_embed = LLM_embedding_matrix[tokenizer.encode('Apple')[-1]]\n",
    "# emb_mat_list = get_llm_embeds(words=['Apple'], add_s=True)['Apple']\n",
    "# calc_embedding_change(og_embed, prompted_next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf4636",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Neuton v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da3277",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_apple = utils.get_image_from_url(\n",
    "    'https://media.istockphoto.com/id/184276818/photo/red-apple.jpg?s=612x612&w=0&k=20&c=NvO-bLsG0DJ_7Ii8SSVoKLurzjmV0Qi4eGfn6nW3l5w='\n",
    ")\n",
    "plt.imshow(inp_image_apple)\n",
    "ve_k = get_visual_embeddings(inp_image_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490a2aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k.numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d9041",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(ve_k.numpy()[0] / np.linalg.norm(ve_k.numpy()[0]), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cd87a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(LLM_embedding_matrix.numpy().reshape(-1), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdacb1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, b = get_embedding_similarity(ve_k / np.linalg.norm(ve_k.numpy()[0]), top_k=-1, verbose=False, low_rank=None, add_fuse_token=None, post_fuse_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2068e6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_b = [f.strip() for f in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64785560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87100fd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fwefw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12351494",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Notes:\n",
    "#### 1) L2 distance is better than cossim --> Embeddings from VIS are unnormalized\n",
    "#### 2) Some words are generally closer to clusters <--> No direct connection as to why \n",
    "#### 3) There might be issues with tokenizer --> No\n",
    "#### 4) Due to size difference any similarity is irrelevant (white image is close to cluster as a meaningful image)\n",
    "#### 5) However it is still meaningful at that scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a899c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_k = utils.get_image_from_url('https://img.freepik.com/premium-vector/cartoon-king-holding-golden-scepter_29190-5435.jpg?w=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704477c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(inp_image_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d880866",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k = get_visual_embeddings(inp_image_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9b0db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_king = copy.deepcopy(ve_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f61716",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_king[(filtered_king > 15) | (filtered_king < -15)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c41d80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_embedding_similarity(filtered_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52dde6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_eucledian_similarity(ve_k, norm=2, top_k=20, verbose=True, low_rank=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a45367",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_embedding_similarity(LLM_embedding_matrix[tokenizer.encode('king')[-1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bb383",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_eucledian_similarity(LLM_embedding_matrix[tokenizer.encode('king')[-1],:][None,...], norm=2, top_k=20, verbose=True, low_rank=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4239a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = get_embedding_similarity(people_centroid) #People\n",
    "get_word_rank(people_centroid, 'human')\n",
    "get_word_rank(people_centroid, 'person')\n",
    "get_word_rank(people_centroid, 'man')\n",
    "get_word_rank(people_centroid, 'woman')\n",
    "# Man > Human > Woman > Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524787b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = get_embedding_similarity(fnv_centroid) # Fruits and Vegetables\n",
    "get_word_rank(fnv_centroid, 'fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4821668",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### A Random image of a king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fca5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_k = utils.get_image_from_url('https://img.freepik.com/premium-vector/cartoon-king-holding-golden-scepter_29190-5435.jpg?w=2000')\n",
    "ve_k = get_visual_embeddings(inp_image_k)\n",
    "_ = get_embedding_similarity(ve_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831694b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Global centroid\n",
    "_ = get_embedding_similarity(global_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa91f41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0bb85",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KING, QUEEN, MAN, WOMAN experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abfc7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_k = utils.get_image_from_url('https://img.freepik.com/premium-vector/cartoon-king-holding-golden-scepter_29190-5435.jpg?w=2000') # King\n",
    "inp_image_m = utils.get_image_from_url('https://iheartcraftythings.com/wp-content/uploads/2021/04/Man-DRAWING-%E2%80%93-STEP-10.jpg') # Man\n",
    "inp_image_p = utils.get_image_from_url('https://img.freepik.com/free-vector/beautiful-queen-cartoon-character-sticker_1308-58920.jpg?w=2000') # Princess\n",
    "inp_image_w = utils.get_image_from_url('https://iheartcraftythings.com/wp-content/uploads/2021/05/Woman-DRAWING-%E2%80%93-STEP-10.jpg') # Woman\n",
    "inp_image_d = utils.get_image_from_url('https://cdn.britannica.com/60/8160-050-08CCEABC/German-shepherd.jpg') # Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ae6a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a44119",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_margin(pil_img, top, right, bottom, left, color):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d86ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f926cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "down_right_dog = add_margin(inp_image_d, 200,0,0,200,'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434b8bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "down_right_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9aace",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258485c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fccc68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6f48a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9438f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Distance before vs after projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514606d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k_no = get_visual_embeddings(inp_image_k, mode='no_projection')[None, ...]\n",
    "ve_m_no = get_visual_embeddings(inp_image_m, mode='no_projection')[None, ...]\n",
    "ve_p_no = get_visual_embeddings(inp_image_p, mode='no_projection')[None, ...]\n",
    "ve_w_no = get_visual_embeddings(inp_image_w, mode='no_projection')[None, ...]\n",
    "ve_d_no = get_visual_embeddings(inp_image_d, mode='no_projection')[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3de59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k = get_visual_embeddings(inp_image_k)\n",
    "ve_m = get_visual_embeddings(inp_image_m)\n",
    "ve_p = get_visual_embeddings(inp_image_p)\n",
    "ve_w = get_visual_embeddings(inp_image_w)\n",
    "ve_d = get_visual_embeddings(inp_image_d)\n",
    "### KING - MAN + WOMAN\n",
    "### PRINCESS - WOMAN + MAN\n",
    "possibly_princess = ve_k - ve_m + ve_w\n",
    "possibly_king = ve_p - ve_w + ve_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966e6d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### With Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759a4fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s = torch.cdist(ve_k,ve_m, p=2)\n",
    "print(f\"King-Man distance:{k_m_s}\")\n",
    "p_m_w = torch.cdist(ve_p,ve_w, p=2)\n",
    "print(f\"Princess-Woman distance:{p_m_w}\")\n",
    "k_p_s = torch.cdist(ve_k,ve_p, p=2)\n",
    "print(f\"King-Princess distance:{k_p_s}\")\n",
    "m_w_s = torch.cdist(ve_m,ve_w, p=2)\n",
    "print(f\"Man-Woman distance:{m_w_s}\")\n",
    "m_p_s = torch.cdist(ve_m,ve_p, p=2)\n",
    "print(f\"Man-Princess distance:{m_p_s}\")\n",
    "w_k_s = torch.cdist(ve_w,ve_k, p=2)\n",
    "print(f\"Woman-King distance:{w_k_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5b690",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "initial_size = 1024\n",
    "projected_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142aa1d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Without Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a97d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s_no = torch.cdist(ve_k_no,ve_m_no, p=2)\n",
    "print(f\"King-Man distance:{k_m_s}\")\n",
    "p_m_w_no = torch.cdist(ve_p_no,ve_w_no, p=2)\n",
    "print(f\"Princess-Woman distance:{p_m_w}\")\n",
    "k_p_s_no = torch.cdist(ve_k_no,ve_p_no, p=2)\n",
    "print(f\"King-Princess distance:{k_p_s}\")\n",
    "m_w_s_no = torch.cdist(ve_m_no,ve_w_no, p=2)\n",
    "print(f\"Man-Woman distance:{m_w_s}\")\n",
    "m_p_s_no = torch.cdist(ve_m_no,ve_p_no, p=2)\n",
    "print(f\"Man-Princess distance:{m_p_s}\")\n",
    "w_k_s_no = torch.cdist(ve_w_no,ve_k_no, p=2)\n",
    "print(f\"Woman-King distance:{w_k_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f745b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(k_m_s / k_m_s_no)\n",
    "print(p_m_w / p_m_w_no)\n",
    "print(k_p_s / k_p_s_no)\n",
    "print(m_w_s / m_w_s_no)\n",
    "print(m_p_s / m_p_s_no)\n",
    "print(w_k_s / w_k_s_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75087cbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f5248",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "k_m_s = torch.nn.CosineSimilarity(dim=1)(ve_k,ve_m)\n",
    "print(f\"King-Man similarity:{k_m_s}\")\n",
    "p_m_w = torch.nn.CosineSimilarity(dim=1)(ve_p,ve_w)\n",
    "print(f\"Princess-Woman similarity:{p_m_w}\")\n",
    "k_p_s = torch.nn.CosineSimilarity(dim=1)(ve_k,ve_p)\n",
    "print(f\"King-Princess similarity:{k_p_s}\")\n",
    "m_w_s = torch.nn.CosineSimilarity(dim=1)(ve_m,ve_w)\n",
    "print(f\"Man-Woman similarity:{k_p_s}\")\n",
    "m_p_s = torch.nn.CosineSimilarity(dim=1)(ve_m,ve_p)\n",
    "print(f\"Man-Princess similarity:{m_p_s}\")\n",
    "w_k_s = torch.nn.CosineSimilarity(dim=1)(ve_w,ve_k)\n",
    "print(f\"Woman-King similarity:{w_k_s}\")\n",
    "print('='*30)\n",
    "k_m_s = torch.cdist(ve_k,ve_m, p=2)\n",
    "print(f\"King-Man distance:{k_m_s}\")\n",
    "p_m_w = torch.cdist(ve_p,ve_w, p=2)\n",
    "print(f\"Princess-Woman distance:{p_m_w}\")\n",
    "k_p_s = torch.cdist(ve_k,ve_p, p=2)\n",
    "print(f\"King-Princess distance:{k_p_s}\")\n",
    "m_w_s = torch.cdist(ve_m,ve_w, p=2)\n",
    "print(f\"Man-Woman distance:{m_w_s}\")\n",
    "m_p_s = torch.cdist(ve_m,ve_p, p=2)\n",
    "print(f\"Man-Princess distance:{m_p_s}\")\n",
    "w_k_s = torch.cdist(ve_w,ve_k, p=2)\n",
    "print(f\"Woman-King distance:{w_k_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5412b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### KING - MAN + WOMAN\n",
    "### PRINCESS - WOMAN + MAN\n",
    "possibly_princess = ve_k - ve_m + ve_w\n",
    "possibly_king = ve_p - ve_w + ve_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2a95d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s = torch.nn.CosineSimilarity(dim=1)(possibly_princess,ve_m)\n",
    "print(f\"Virtual Princess - Man similarity:{k_m_s}\")\n",
    "p_m_w = torch.nn.CosineSimilarity(dim=1)(possibly_princess,ve_w)\n",
    "print(f\"Virtual Princess - Woman similarity:{p_m_w}\")\n",
    "k_p_s = torch.nn.CosineSimilarity(dim=1)(possibly_princess,ve_p)\n",
    "print(f\"Virtual Princess - Princess similarity:{k_p_s}\")\n",
    "k_p_s = torch.nn.CosineSimilarity(dim=1)(possibly_princess,ve_k)\n",
    "print(f\"Virtual Princess - King similarity:{k_p_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cd6c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s = torch.cdist(possibly_princess,ve_m, p=2)\n",
    "print(f\"Virtual Princess - Man distance:{k_m_s}\")\n",
    "p_m_w = torch.cdist(possibly_princess,ve_w, p=2)\n",
    "print(f\"Virtual Princess - Woman distance:{p_m_w}\")\n",
    "k_p_s = torch.cdist(possibly_princess,ve_p, p=2)\n",
    "print(f\"Virtual Princess - Princess distance:{k_p_s}\")\n",
    "k_p_s = torch.cdist(possibly_princess,ve_k, p=2)\n",
    "print(f\"Virtual Princess - King distance:{k_p_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee2917",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s = torch.nn.CosineSimilarity(dim=1)(possibly_king,ve_m)\n",
    "print(f\"Virtual King - Man similarity:{k_m_s}\")\n",
    "p_m_w = torch.nn.CosineSimilarity(dim=1)(possibly_king,ve_w)\n",
    "print(f\"Virtual King - Woman similarity:{p_m_w}\")\n",
    "k_p_s = torch.nn.CosineSimilarity(dim=1)(possibly_king,ve_p)\n",
    "print(f\"Virtual King - Princess similarity:{k_p_s}\")\n",
    "k_p_s = torch.nn.CosineSimilarity(dim=1)(possibly_king,ve_k)\n",
    "print(f\"Virtual King - King similarity:{k_p_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e141a31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_m_s = torch.cdist(possibly_king,ve_m, p=2)\n",
    "print(f\"Virtual King - Man distance:{k_m_s}\")\n",
    "p_m_w = torch.cdist(possibly_king,ve_w, p=2)\n",
    "print(f\"Virtual King - Woman distance:{p_m_w}\")\n",
    "k_p_s = torch.cdist(possibly_king,ve_p, p=2)\n",
    "print(f\"Virtual King - Princess distance:{k_p_s}\")\n",
    "k_p_s = torch.cdist(possibly_king,ve_k, p=2)\n",
    "print(f\"Virtual King - King distance:{k_p_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa4cd1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d53f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trunc_caption(caption: str) -> str:\n",
    "    # Truncate at period.\n",
    "    trunc_index = caption.find('.') + 1\n",
    "    if trunc_index < 0:\n",
    "        trunc_index = caption.find('\\n') + 1\n",
    "    caption = caption[:trunc_index]\n",
    "    return caption\n",
    "\n",
    "def display_interleaved_outputs(model_outputs, one_img_per_ret=True):\n",
    "    for output in model_outputs:\n",
    "        if type(output) == str:\n",
    "            print(output)\n",
    "        elif type(output) == list:\n",
    "            if one_img_per_ret:\n",
    "                plt.figure(figsize=(3, 3))\n",
    "                plt.imshow(np.array(output[0]))\n",
    "            else:\n",
    "                fig, ax = plt.subplots(1, len(output), figsize=(3 * len(output), 3))\n",
    "                for i, image in enumerate(output):\n",
    "                    image = np.array(image)\n",
    "                    ax[i].imshow(image)\n",
    "                    ax[i].set_title(f'Retrieval #{i+1}')\n",
    "            plt.show()\n",
    "        elif type(output) == Image.Image:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(np.array(output))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79654",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiment with Markos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b8482",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"I like ice cream\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f564ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e3e81",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LLM_embedding_matrix[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb595d5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17c97f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### End of Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc43f8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a7f62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('King')\n",
    "plt.scatter(range(ve_k.size()[1]), ve_k.numpy()[0,:], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9deb3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Man?')\n",
    "plt.scatter(range(ve_m.size()[1]), ve_m.numpy()[0,:], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397f62f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Queen')\n",
    "plt.scatter(range(ve_p.size()[1]), ve_p.numpy()[0,:], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818874ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Woman?')\n",
    "plt.scatter(range(ve_w.size()[1]), ve_w.numpy()[0,:], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f98127",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Dog')\n",
    "plt.scatter(range(ve_d.size()[1]), ve_d.numpy()[0,:], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83135274",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Dog: {torch.argsort(ve_d, descending=True)}\")\n",
    "print(f\"Man: {torch.argsort(ve_m, descending=True)}\")\n",
    "print(f\"Woman: {torch.argsort(ve_w, descending=True)}\")\n",
    "print(f\"King: {torch.argsort(ve_k, descending=True)}\")\n",
    "print(f\"Queen: {torch.argsort(ve_p, descending=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288d1af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_king = copy.deepcopy(ve_k.numpy()[0,:])\n",
    "#filtered_king[(filtered_king > -0.1) & (filtered_king < 0.1)] = 0\n",
    "filtered_king[(filtered_king > 20)] = 0\n",
    "plt.figure()\n",
    "plt.title('Filtered King with no ROBE')\n",
    "plt.scatter(range(ve_k.size()[1]), filtered_king, alpha=0.9, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40831705",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_queen = copy.deepcopy(ve_p.numpy()[0,:])\n",
    "#filtered_king[(filtered_king > -0.1) & (filtered_king < 0.1)] = 0\n",
    "filtered_queen[(filtered_queen > 10) | (filtered_queen < -10)] = 0\n",
    "plt.figure()\n",
    "plt.title('Filtered Queen with no ROBE')\n",
    "plt.scatter(range(ve_p.size()[1]), filtered_queen, alpha=0.9, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025c19e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_dog = copy.deepcopy(ve_d.numpy()[0,:])\n",
    "#filtered_king[(filtered_king > -0.1) & (filtered_king < 0.1)] = 0\n",
    "filtered_dog[(filtered_dog > 20) | (filtered_dog < -20)] = 0\n",
    "plt.figure()\n",
    "plt.title('Filtered Dog')\n",
    "plt.scatter(range(ve_d.size()[1]), filtered_dog, alpha=0.9, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ba9c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filtered_king_tensor = torch.Tensor(filtered_king)[None, ...]\n",
    "filtered_queen_tensor = torch.Tensor(filtered_queen)[None, ...]\n",
    "filtered_dog_tensor = torch.Tensor(filtered_dog)[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177deff8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ve_k = get_visual_embeddings(inp_image_k)\n",
    "ve_m = get_visual_embeddings(inp_image_m)\n",
    "ve_p = get_visual_embeddings(inp_image_p)\n",
    "ve_w = get_visual_embeddings(inp_image_w)\n",
    "ve_d = get_visual_embeddings(inp_image_d)\n",
    "ve_d2 = get_visual_embeddings(down_right_dog)\n",
    "### KING - MAN + WOMAN\n",
    "### PRINCESS - WOMAN + MAN\n",
    "possibly_princess = ve_k - ve_m + ve_w\n",
    "possibly_king = ve_p - ve_w + ve_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e4c44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp_image_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162ce4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crownless_person = ve_k - ve_p + ve_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887a59d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(ve_m + ve_d).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0f074",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(ve_m + ve_d2).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb2720",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "animal_centroid.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b9e9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mix = (animal_centroid + people_centroid) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd9a84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for scale in [1]:\n",
    "    #rescale = ((ve_m + scale * ve_d2).norm() / ((ve_m.norm() + (scale * ve_d2).norm()) / 2))\n",
    "#     rescale = 1\n",
    "#     crownless_person_with_dog = (ve_m + scale * ve_d2) / rescale\n",
    "    #print(f\"Scale: {scale}\\n\")\n",
    "    for inp_text in ['Picture of a']:\n",
    "        prompts = [mix[None,...],inp_text]\n",
    "        print('Prompt:')\n",
    "        display_interleaved_outputs(prompts)\n",
    "        print('=' * 30)\n",
    "\n",
    "        input_embs = []\n",
    "        add_bos = True\n",
    "\n",
    "        for i, p in enumerate(prompts):\n",
    "          if type(p) == Image.Image:\n",
    "            # Encode as image.\n",
    "            pixel_values = utils.get_pixel_values_for_model(model.feature_extractor, p)\n",
    "            pixel_values = pixel_values.to(device=model.logit_scale.device, dtype=model.logit_scale.dtype)\n",
    "            pixel_values = pixel_values[None, ...]\n",
    "\n",
    "            visual_embs = model.get_visual_embs(pixel_values, mode='captioning')  # (1, n_visual_tokens, D)\n",
    "            input_embs.append(visual_embs)\n",
    "          elif type(p) == str:\n",
    "            text_ids = model.tokenizer(p, add_special_tokens=True, return_tensors=\"pt\").input_ids.to(model.logit_scale.device)\n",
    "            if not add_bos:\n",
    "              # Remove <bos> tag.\n",
    "              text_ids = text_ids[:, 1:]\n",
    "            else:\n",
    "              # Only add <bos> once.\n",
    "              add_bos = False\n",
    "\n",
    "            text_embs = model.input_embeddings(text_ids)  # (1, T, D)\n",
    "            input_embs.append(text_embs)\n",
    "          elif type(p) == torch.Tensor:\n",
    "              input_embs.append(p[None, ...])\n",
    "          else:\n",
    "            raise ValueError(f'Input prompts should be either PIL.Image.Image or str types, got {type(p)} instead.')\n",
    "        input_embs = torch.cat(input_embs, dim=1)\n",
    "        for _ in range(1):\n",
    "            generated_ids, generated_embeddings, _ = model.generate(input_embs, 20, temperature=0.0, top_p=1 ,ret_scale_factor=1)\n",
    "            embeddings = generated_embeddings[-1][:, input_embs.shape[1]:]\n",
    "\n",
    "            newline_token_id = model.tokenizer('\\n', add_special_tokens=False).input_ids[0]\n",
    "            trunc_idx = 0\n",
    "            for j in range(generated_ids.shape[1]):\n",
    "                if generated_ids[0, j] == newline_token_id:\n",
    "                    trunc_idx = j\n",
    "                    break\n",
    "            if trunc_idx > 0:\n",
    "                generated_ids = generated_ids[:, :trunc_idx]\n",
    "                embeddings = embeddings[:, :trunc_idx]\n",
    "\n",
    "            return_outputs = []\n",
    "            caption = model.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return_outputs.append(utils.truncate_caption(caption))\n",
    "            print(return_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cdccc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41810838",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d0c67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.encode('king of the castle in a red robe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b8c4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "description = np.zeros((9, 4096))\n",
    "for i,v  in enumerate([2, 7037, 9, 5, 22637, 11, 10, 1275, 36393]):\n",
    "    description[i] = LLM_embedding_matrix[v,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27deb478",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb249af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.nn.CosineSimilarity(dim=1)(ve_k,torch.Tensor(description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdd483",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Looking at the Linear Projection Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70a304",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = checkpoint['state_dict']['visual_embeddings.weight']\n",
    "b = checkpoint['state_dict']['visual_embeddings.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed724c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A_ =  A.float().numpy()\n",
    "b_ = b.float().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5fcc5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e6ab5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(A_.T,ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cc8c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(b_, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30363347",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb634ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw=None, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if cbar_kw is None:\n",
    "        cbar_kw = {}\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e5b5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3dcc5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unembed = model.lm.lm_head.weight.detach().cpu()\n",
    "unembed.requires_grad = False\n",
    "unembed = unembed.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(50267, 4096)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unembed.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "embeds = model.lm.model.decoder.embed_tokens.weight.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(50267, 4096)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "unembed =  unembed.astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "embeds =  embeds.astype('float32')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from numba import jit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def matmul(a, b):\n",
    "    return a @ b.T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def scale_mat(a):\n",
    "    row_max = np.max(a, axis=1)\n",
    "    row_min = np.min(a, axis=1)\n",
    "    row_range = row_max - row_min\n",
    "    return np.divide(a - row_min[..., None], row_range[...,None])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "corrmat = matmul(embeds, unembed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array([49955, 49955,     2, ..., 49955, 49955, 49955], dtype=int64)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(corrmat,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "MAGIC_NUMBER = 49955"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "n = np.linalg.norm(embeds, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "2.5191686"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6344778"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "(50267,)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n          0.,   0.,   0.,   1.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,\n          0.,   2.,   1.,   0.,   2.,   0.,   0.,   0.,   0.,   1.,   1.,\n          1.,   1.,   1.,   0.,   0.,   0.,   1.,   1.,   0.,   1.,   1.,\n          1.,   1.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,\n          1.,   2.,   1.,   1.,   0.,   1.,   1.,   1.,   0.,   0.,   0.,\n          1.,   0.,   1.,   1.,   1.,   3.,   1.,   0.,   1.,   1.,   0.,\n          0.,   1.,   2.,   0.,   1.,   1.,   1.,   2.,   1.,   1.,   0.,\n          3.,   2.,   2.,   0.,   2.,   1.,   1.,   2.,   2.,   4.,   1.,\n          3.,   6.,   4.,   5.,   2.,   3.,   5.,   8.,   1.,   3.,   4.,\n          6.,   4.,   4.,   3.,   2.,   3.,   5.,   4.,   3.,   2.,   8.,\n          2.,   4.,   1.,   4.,   9.,   4.,   3.,   3.,   7.,   3.,   3.,\n          4.,  10.,   6.,   7.,   5.,   4.,   5.,  12.,   1.,   8.,   8.,\n          9.,   4.,   5.,   5.,  17.,   8.,   6.,  10.,   7.,   6.,   9.,\n         10.,   5.,  15.,  11.,  13.,   5.,  10.,   8.,  14.,  10.,   7.,\n         11.,  10.,  13.,   9.,   8.,   9.,  10.,  14.,  11.,  16.,  14.,\n         21.,  16.,  11.,  15.,  15.,  10.,  11.,  11.,  11.,  16.,  20.,\n         19.,  15.,  25.,  12.,  20.,  15.,  23.,  13.,  21.,  16.,  16.,\n         23.,  17.,  27.,  15.,  16.,  25.,  24.,  23.,  20.,  28.,  21.,\n         17.,  23.,  26.,  25.,  22.,  25.,  36.,  30.,  34.,  30.,  18.,\n         24.,  46.,  29.,  38.,  35.,  39.,  33.,  34.,  37.,  34.,  33.,\n         41.,  36.,  35.,  48.,  37.,  40.,  49.,  42.,  39.,  50.,  47.,\n         53.,  44.,  47.,  48.,  51.,  54.,  42.,  47.,  50.,  48.,  53.,\n         62.,  47.,  47.,  56.,  54.,  56.,  71.,  59.,  41.,  57.,  47.,\n         63.,  48.,  55.,  66.,  53.,  68.,  47.,  57.,  76.,  64.,  70.,\n         75.,  78.,  64.,  80.,  68.,  72.,  60.,  61.,  94.,  81.,  69.,\n         83.,  80.,  66.,  66.,  88.,  81.,  77.,  81.,  78.,  93.,  76.,\n         96.,  96.,  84., 111.,  78.,  83., 115.,  96.,  91.,  87., 110.,\n        113.,  92., 117., 104., 101.,  94., 103., 113.,  98., 104., 125.,\n        112., 112., 115., 115., 118., 117., 127., 107., 122., 107.,  99.,\n        117., 118., 125., 113., 110., 140., 112.,  98., 124., 121., 141.,\n        134., 103., 119., 146., 139., 146., 158., 127., 134., 158., 128.,\n        156., 134., 151., 150., 134., 159., 159., 148., 143., 160., 167.,\n        147., 146., 164., 132., 161., 182., 167., 157., 149., 175., 170.,\n        177., 174., 161., 160., 166., 148., 173., 154., 167., 161., 140.,\n        166., 183., 155., 163., 178., 165., 181., 169., 161., 169., 172.,\n        159., 164., 159., 168., 172., 171., 157., 181., 173., 191., 176.,\n        190., 167., 179., 166., 170., 180., 187., 176., 179., 198., 183.,\n        199., 190., 181., 172., 166., 169., 157., 187., 166., 200., 171.,\n        218., 177., 182., 160., 178., 189., 189., 170., 162., 189., 165.,\n        163., 183., 197., 213., 175., 190., 178., 185., 166., 188., 177.,\n        183., 184., 162., 186., 175., 174., 189., 179., 171., 151., 160.,\n        176., 198., 187., 171., 156., 180., 149., 153., 167., 156., 157.,\n        186., 171., 174., 172., 153., 163., 166., 180., 160., 142., 156.,\n        172., 155., 164., 164., 161., 146., 159., 160., 152., 154., 172.,\n        140., 146., 146., 162., 166., 168., 156., 139., 164., 131., 139.,\n        128., 146., 127., 159., 142., 129., 148., 105., 121., 105., 122.,\n        121., 128., 133., 144., 126., 115., 132., 122., 132., 120., 126.,\n        120., 135., 142., 122., 119., 114., 114., 124., 105., 112., 103.,\n        119., 106., 121., 118., 115., 123., 123.,  94., 128., 100., 104.,\n        102., 117., 124.,  61.,  83.,  95.,  84., 106., 101.,  82.,  89.,\n        106.,  84., 105.,  83.,  83.,  73.,  80.,  80.,  77.,  89.,  70.,\n         82.,  78.,  78.,  73.,  57.,  66.,  89.,  71.,  80.,  82.,  71.,\n         59.,  65.,  65.,  65.,  52.,  67.,  65.,  72.,  66.,  67.,  47.,\n         65.,  58.,  59.,  62.,  52.,  52.,  55.,  65.,  53.,  40.,  45.,\n         55.,  45.,  43.,  47.,  48.,  48.,  48.,  45.,  50.,  41.,  45.,\n         46.,  49.,  46.,  32.,  36.,  40.,  47.,  43.,  34.,  29.,  48.,\n         37.,  42.,  35.,  33.,  39.,  31.,  25.,  39.,  26.,  33.,  39.,\n         37.,  33.,  32.,  27.,  32.,  22.,  25.,  27.,  18.,  23.,  25.,\n         27.,  22.,  19.,  29.,  27.,  19.,  20.,  18.,  19.,  21.,  18.,\n         16.,  31.,  25.,  26.,  19.,  17.,  19.,  23.,  17.,  23.,  20.,\n         14.,  20.,  18.,  19.,  11.,  20.,  16.,  18.,  14.,  13.,  17.,\n         12.,   9.,  12.,  10.,  17.,  14.,  13.,  11.,  12.,   7.,  10.,\n         14.,   8.,   9.,  11.,   7.,   9.,   6.,   9.,   9.,  10.,   8.,\n         10.,   9.,   9.,   6.,   9.,   4.,   8.,   8.,   4.,   2.,   7.,\n         12.,   5.,   4.,   7.,   8.,   1.,   5.,   4.,   8.,   5.,   8.,\n          9.,   5.,   3.,   2.,   4.,   2.,   3.,   5.,   5.,   5.,   3.,\n          1.,   2.,   8.,   2.,   3.,   7.,   6.,   2.,   1.,   6.,   0.,\n          5.,   3.,   3.,   2.,   4.,   1.,   2.,   1.,   7.,   3.,   0.,\n          2.,   3.,   3.,   0.,   2.,   0.,   2.,   2.,   2.,   2.,   1.,\n          2.,   0.,   1.,   1.,   3.,   1.,   3.,   0.,   2.,   2.,   2.,\n          0.,   1.,   1.,   1.,   0.,   1.,   0.,   2.,   4.,   0.,   1.,\n          1.,   0.,   0.,   0.,   1.,   2.,   0.,   0.,   1.,   2.,   0.,\n          0.,   0.,   3.,   0.,   0.,   1.,   0.,   0.,   2.,   0.,   1.,\n          1.,   0.,   1.,   1.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   0.,\n          0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n          0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,\n          1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n array([0.81412032, 0.81547164, 0.81682295, ..., 2.16273333, 2.16408464,\n        2.16543596]),\n <BarContainer object of 1000 artists>)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(n, bins=1000, color='blue', alpha=0.8)\n",
    "plt.hist(np.random.normal(loc=1.5, scale=0.15, size=50267), bins=1000, color='red', alpha=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "data": {
      "text/plain": "FromageModel(\n  (lm): None\n  (input_embeddings): Embedding(50267, 4096)\n  (visual_model): CLIPVisionModel(\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n        (position_embedding): Embedding(257, 1024)\n      )\n      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (12): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (13): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (14): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (15): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (16): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (17): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (18): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (19): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (20): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (21): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (22): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n          (23): CLIPEncoderLayer(\n            (self_attn): CLIPAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            )\n            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (text_hidden_fcs): ModuleList()\n  (visual_embeddings): Linear(in_features=1024, out_features=4096, bias=True)\n  (visual_fc): Linear(in_features=1024, out_features=256, bias=True)\n  (image_dropout): Dropout(p=0.0, inplace=False)\n)"
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "inp_image_d = utils.get_image_from_url('https://cdn.britannica.com/60/8160-050-08CCEABC/German-shepherd.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "another_dog = utils.get_image_from_url('https://ichef.bbci.co.uk/news/976/cpsprodpb/17638/production/_124800859_gettyimages-817514614.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "another_dog_no_project = get_visual_embeddings(another_dog, mode='no_projection')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "cat = utils.get_image_from_url('https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg')\n",
    "cat_no_project = get_visual_embeddings(cat, mode='no_projection')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "model.lm = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "ve_d_no_project = get_visual_embeddings(inp_image_d, mode='no_projection')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "ve_d_no_project = ve_d_no_project[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-3.0248,  4.0206,  0.9249,  ..., -0.2175, -0.4823, -0.3924])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve_d_no_project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "prompt = 'german shepherd dog animal'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "targets = tokenizer.encode(prompt, return_tensors='pt')[0,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "sparse_targets = torch.zeros(50267).scatter_(0, targets, torch.ones(50267)).to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "unembed_param = torch.nn.Parameter(data=torch.Tensor(unembed), requires_grad=False).to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50267, 4096])"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unembed_param.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "linear_projection = torch.nn.Linear(in_features=1024, out_features=4096, bias=True, device='cuda')\n",
    "clever_ln = torch.nn.LayerNorm(normalized_shape=4096, elementwise_affine=True, device='cuda')\n",
    "loss_fn = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params= [\n",
    "        {'params': linear_projection.parameters() , 'lr': 0.001, 'weight_decay': 0.001},\n",
    "        {'params': clever_ln.parameters(), 'lr': 0.001, 'weight_decay': 0}\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:99 L1 Norm: nan Loss:nan\n",
      "Step:199 L1 Norm: nan Loss:nan\n",
      "Step:299 L1 Norm: nan Loss:nan\n",
      "Step:399 L1 Norm: nan Loss:nan\n",
      "Step:499 L1 Norm: nan Loss:nan\n",
      "Step:599 L1 Norm: nan Loss:nan\n",
      "Step:699 L1 Norm: nan Loss:nan\n",
      "Step:799 L1 Norm: nan Loss:nan\n",
      "Step:899 L1 Norm: nan Loss:nan\n",
      "Step:999 L1 Norm: nan Loss:nan\n",
      "Step:1099 L1 Norm: nan Loss:nan\n",
      "Step:1199 L1 Norm: nan Loss:nan\n",
      "Step:1299 L1 Norm: nan Loss:nan\n",
      "Step:1399 L1 Norm: nan Loss:nan\n",
      "Step:1499 L1 Norm: nan Loss:nan\n",
      "Step:1599 L1 Norm: nan Loss:nan\n",
      "Step:1699 L1 Norm: nan Loss:nan\n",
      "Step:1799 L1 Norm: nan Loss:nan\n",
      "Step:1899 L1 Norm: nan Loss:nan\n",
      "Step:1999 L1 Norm: nan Loss:nan\n",
      "Step:2099 L1 Norm: nan Loss:nan\n",
      "Step:2199 L1 Norm: nan Loss:nan\n",
      "Step:2299 L1 Norm: nan Loss:nan\n",
      "Step:2399 L1 Norm: nan Loss:nan\n",
      "Step:2499 L1 Norm: nan Loss:nan\n",
      "Step:2599 L1 Norm: nan Loss:nan\n",
      "Step:2699 L1 Norm: nan Loss:nan\n",
      "Step:2799 L1 Norm: nan Loss:nan\n",
      "Step:2899 L1 Norm: nan Loss:nan\n",
      "Step:2999 L1 Norm: nan Loss:nan\n",
      "Step:3099 L1 Norm: nan Loss:nan\n",
      "Step:3199 L1 Norm: nan Loss:nan\n",
      "Step:3299 L1 Norm: nan Loss:nan\n",
      "Step:3399 L1 Norm: nan Loss:nan\n",
      "Step:3499 L1 Norm: nan Loss:nan\n",
      "Step:3599 L1 Norm: nan Loss:nan\n"
     ]
    },
    {
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_vec = ve_d_no_project.to('cuda')\n",
    "for step in range(0,5000):\n",
    "    optimizer.zero_grad()\n",
    "    # Project Image #\n",
    "    a = linear_projection(image_vec)\n",
    "    a = clever_ln(a)\n",
    "    #a = clever_ln(a)\n",
    "    # Projected Image Must be Matched to Unembeddings #\n",
    "    emb_a = unembed_param @ a\n",
    "    # Calculate Loss #\n",
    "    loss = loss_fn(emb_a, sparse_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 100 == 99:\n",
    "        print(f\"Step:{step} L1 Norm: {a.norm(p=1)} Loss:{loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ax = linear_projection(image_vec)\n",
    "    ax = clever_ln(ax)\n",
    "    emb_a = unembed_param @ ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(emb_a.argsort(descending=True)[0:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ax = linear_projection(another_dog_no_project.to('cuda')[0])\n",
    "    ax = clever_ln(ax)\n",
    "    emb_a = unembed_param @ ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(emb_a.argsort(descending=True)[0:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ax = linear_projection(cat_no_project.to('cuda')[0])\n",
    "    ax = clever_ln(ax)\n",
    "    emb_a = unembed_param @ ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(emb_a.argsort(descending=True)[0:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ax = linear_projection(torch.randn(1024).to('cuda'))\n",
    "    ax = clever_ln(ax)\n",
    "    emb_a = unembed_param @ ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(emb_a.argsort(descending=True)[0:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}